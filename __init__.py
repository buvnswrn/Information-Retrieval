# -*- coding: utf-8 -*-
"""project_part2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t81RRVZU1qS8x2BT3ZZ7g98DrudE0Dl0
"""

from google.colab import drive
import os
drive.mount('/content/drive')
os.chdir("drive/My Drive/ColabNotebooks")

from nltk.tokenize import RegexpTokenizer
import string as STR
from collections import Counter
import math
from tqdm import tqdm
from bs4 import BeautifulSoup
from nltk.tokenize import sent_tokenize
import re
import nltk
nltk.download('punkt')
from sklearn.svm import LinearSVC
import itertools
import numpy as np

def retrieval_tfidf():
    score = {}
    magnitude_doc = {}
    for query in tqdm(queries.keys()):
        term_freq_query = queries[query]
        tfidf_query = {}
        for term in term_freq_query.keys():
            tfidf_query[term] = term_freq_query[term]*idf.get(term,0)
        temp = {}
        magnitude_query = math.sqrt(sum([math.pow(tfidf_query[term],2) for term in tfidf_query.keys() ])) 
        for doc in corpus.keys():
            similarity_dot_product = 0
            for term in tfidf_query.keys():
                tfidf_term_doc = term_frequency.get(doc,{}).get(term,0)*idf.get(term,0)
                similarity_dot_product += tfidf_term_doc * tfidf_query[term]
            if doc not in magnitude_doc.keys():
                magnitude = 0
                for term in term_frequency[doc].keys():
                    magnitude += math.pow(term_frequency[doc][term]*idf[term],2)
                magnitude_doc[doc] = math.sqrt(magnitude)
           
            temp[doc] = similarity_dot_product/(magnitude_doc[doc]*magnitude_query)
        score[query] = temp
    return score

def precision(score):
    precision_50 = {}
    for query in tqdm(score.keys()):
        num_relevant = 0
        for doc,_ in score[query]:
            if any (re.search(regex,corpus_raw[doc],re.IGNORECASE) for regex in query_answer[query]):
                num_relevant+=1
        #print('number of relevant docs for query {} are {}'.format(query,num_relevant))
        precision_50[query] = num_relevant/50.0
    return precision_50

def retrieval_BM25():
    score = {}
    for query in tqdm(queries.keys()):
        temp = {}
        for doc,_ in score_1000[query]:
          
          temp[doc] = 0
          for term in queries[query].keys():
              numerator_constant = PARAM_K1 + 1
              denominator_constant = PARAM_K1 * (1 - PARAM_B + PARAM_B * len(corpus[doc]) / avg_dl)
              temp[doc] +=  idf_bm25.get(term,0)*term_frequency_unnormalized.get(doc,{}).get(term,0)*numerator_constant/(term_frequency_unnormalized.get(doc,{}).get(term,0)+denominator_constant)
        score[query] = temp
    return score

def retrieval_BM25_sentence(sentence_raw,sentence_tokenized,avg_sl):
  score = {}
  for query in tqdm(queries.keys()):
   
    temp = {}
    for index  in sentence_tokenized[query].keys():
        sentence = sentence_tokenized[query][index]
        term_frequency_unnormalized_sentence = Counter(sentence)
        temp[index] = 0
        for term in queries[query].keys():
            numerator_constant = PARAM_K1 + 1
            denominator_constant = PARAM_K1 * (1 - PARAM_B + PARAM_B * len(sentence) / avg_sl)
            temp[index] +=  idf_bm25.get(term,0)*term_frequency_unnormalized_sentence.get(term,0)*numerator_constant/(term_frequency_unnormalized_sentence.get(term,0)+denominator_constant)
    score[query] = temp
  return score

def retrieval_SVM():
  score_svm_50 = {}
  for query in score_1000.keys():
      X = []
      temp =[]
      index_doc = {}
      for doc_id,_ in score_1000[query]:
          feauture_tfidf = 0
          feature_tf = 0
          for term in queries[query].keys():
              feauture_tfidf+=  term_frequency.get(doc_id,{}).get(term,0)*idf.get(term,0)/len(queries[query].keys())
              feature_tf += term_frequency.get(doc_id,{}).get(term,0)/len(queries[query].keys())
          feature_dl = len(corpus[doc_id])/avg_dl
          feature_BM25 =  score_bm25[query][doc_id]
        
          X.append([feature_dl,feature_tf,feauture_tfidf,feature_BM25])
          index_doc[len(X)-1] = doc_id
      result = clf.decision_function(X)
      result_indices = np.argsort(-1*result)[:50]
      for index in result_indices:
          temp.append((index_doc[index],result[index])) 
      score_svm_50[query] = temp
  return score_svm_50

def retrieval_SVM_sentence(sentence_raw,sentence_tokenized,avg_sl,score_sentence_BM25):
  score_svm_50 = {}
  for query in tqdm(queries.keys()):
      X = []
      temp =[]
      index_sentence = {}
      for sentence_id in sentence_tokenized[query].keys():
          sentence = sentence_tokenized[query][sentence_id]
          var = Counter(sentence)
          term_frequency_sentence = {key:var[key]/var.most_common(1)[0][1] for key in var}
          feauture_tfidf = 0
          feature_tf = 0
          for term in queries[query].keys():
              feauture_tfidf+=  term_frequency_sentence.get(term,0)*idf.get(term,0)/len(queries[query].keys())
              feature_tf += term_frequency_sentence.get(term,0)/len(queries[query].keys())
          feature_sl = len(sentence)/avg_sl
          feature_BM25 = score_sentence_BM25[query][sentence_id]
        
          X.append([feature_sl,feature_tf,feauture_tfidf,feature_BM25])
          index_sentence[len(X)-1] = sentence_id
      result = clf.decision_function(X)
      result_indices = np.argsort(-1*result)[:50]
      for index in result_indices:
          temp.append((index_sentence[index],result[index])) 
      score_svm_50[query] = temp
  return score_svm_50

def MRR(score_sentence_50):
  mrr = 0.0
  for query in tqdm(queries.keys()):
    
    
    for tuples,rank in zip(score_sentence_50[query],list(range(1,len(score_sentence_50[query])+1))):
      id = tuples[0]
      
      if any (re.search(regex,sentence_raw[query][id],re.IGNORECASE) for regex in query_answer[query]):
        mrr += 1.0/rank
        print('\nRank of sentence for query {} is {}.'.format(query,rank))
        break
  return float(mrr/len(list(queries.keys())))
  
def transform(X,y):
    X_new = []
    y_new = []
    y = np.asarray(y)
    #index_comb = {}
    if y.ndim == 1:
        y = np.c_[y, np.ones(y.shape[0])]
    comb = itertools.combinations(range(X.shape[0]), 2)
    for k, (i, j) in enumerate(comb):
        if y[i, 0] == y[j, 0] or y[i, 1] != y[j, 1]:
        # skip if same target or different group
          continue
        X_new.append(X[i] - X[j])
        #index_comb[len(X_new)-1] = (i,j)
        y_new.append(np.sign(y[i, 0] - y[j, 0]))
      # output balanced classes
        if y_new[-1] != (-1) ** k:
            y_new[-1] = - y_new[-1]
            X_new[-1] = - X_new[-1]
            #index_comb[len(X_new)-1] = (j,i)
    return np.asarray(X_new), np.asarray(y_new).ravel()#, index_comb

def get_sentence(score):
    sentence_raw= {}
    sentence_tokenized = {}
    for query in tqdm(score.keys()):
      temp = []
      sentence_raw[query] = {}
      for doc,_ in score[query]:
        temp += sent_tokenize(corpus_raw[doc].strip())
      for id,sentence in zip(list(range(1,len(temp)+1)),temp):
        sentence_raw[query][id] = sentence 
      sentence_tokenized[query] = {}
      for id in sentence_raw[query].keys():
        sentence = sentence_raw[query][id]
        temp =  sentence.lower().translate(table)
        temp = tokenizer.tokenize(temp)
        sentence_tokenized[query][id] = temp
    #Calculation of average sentence length:
    total_sentence_length = 0
    num_of_sentences = 0
    for query in tqdm(sentence_tokenized.keys()):
      for id in sentence_tokenized[query].keys():
        total_sentence_length+=len(sentence_tokenized[query][id])
        num_of_sentences+=len(sentence_tokenized[query].keys())
    return sentence_raw,sentence_tokenized,total_sentence_length/(num_of_sentences*1.0)

with open('trec_documents.xml', 'r') as f:  # Reading file
    xml = f.read()
xml = '<ROOT>' + xml + '</ROOT>'
root = BeautifulSoup(xml, 'lxml-xml')
corpus = {}

for doc in root.find_all('DOC'):
    if not doc.find('DOCNO').text.strip().startswith("LA"):
        corpus[doc.find('DOCNO').text.strip()] = doc.find('TEXT').text.strip()
    else:
        text = ""
        for child in  doc.find('TEXT').findChildren("P" , recursive=False):
          
            string = child.text.strip()
            text = ' '.join([text, string])

        corpus[doc.find('DOCNO').text.strip()] = text.strip()
# Dictionary corpus_raw contains raw document(without any pre-processing) as value of corpus_raw[doc_id]. This will be used for 
# pattern matching to find relevance later on.
corpus_raw = corpus.copy()

tokenizer = RegexpTokenizer(r'\w+')
table = str.maketrans('','','!\"#$%&\'()*+,./:;<=>?@[\]^_`{|}~')
corpus_combined = []
term_frequency = {}
data = []
term_frequency_unnormalized = {}
doc_to_index = {doc_id:index for doc_id,index in zip(list(corpus.keys()),list(range(0,len(list(corpus.keys())))))}
for doc in tqdm(corpus.keys()):
    temp = corpus[doc].lower().translate(table)
    temp = tokenizer.tokenize(temp)
    corpus[doc] = temp
    corpus_combined+= corpus[doc]
    data.append(temp)
    #corpus_raw[doc] = ' '.join(corpus[doc])
    term_frequency_unnormalized[doc] = Counter(corpus[doc])
    term_frequency[doc] = {key:term_frequency_unnormalized[doc][key]/term_frequency_unnormalized[doc].most_common(1)[0][1] for key in term_frequency_unnormalized[doc]}

vocab = set(corpus_combined)
Total_docs = len(corpus.keys())
idf = {}
idf_bm25 = {}
negative_idfs = {}
PARAM_K1 = 1.5
PARAM_B = 0.75
EPSILON = 0.25
# collect idf sum to calculate an average idf for epsilon value
idf_sum = 0

for term in tqdm(vocab):
  count_term = 0
  
  for doc in corpus.keys():
    if term in term_frequency[doc]:
      count_term+=1
    #temp[doc] = corpus[doc].get(term,0)
  idf[term] = math.log(Total_docs/count_term)
  idf_bm25[term] = math.log(Total_docs - count_term + 0.5) - math.log(count_term + 0.5)
  idf_sum += idf_bm25[term]
  if idf_bm25[term] < 0:
    negative_idfs[term] = 'random value'
  #term_frequency[term] = temp
average_idf = float(idf_sum) / len(list(idf_bm25.keys()))
eps = EPSILON * average_idf
for term in tqdm(negative_idfs):
  idf_bm25[term] = eps

queries = {}
queries_bow = {}
with open('test_questions.txt', 'r') as f:  
    file = f.read()


root = BeautifulSoup(file, 'lxml')
for num, query in zip(list(range(1,101)),root.body.find_all('top')):
  text =  ' '.join(query.num.desc.text.strip().split()[1:])
  text = text.lower().translate(table)
  text = tokenizer.tokenize(text)
  queries_bow[num] = text
  temp = Counter(text)
  queries[num] = {key:temp[key]/temp.most_common(1)[0][1] for key in temp}

score = retrieval_tfidf()
score_50 = {}
score_1000 = {}
for key in score.keys():
  score[key]  =sorted(score[key].items(), key=lambda x: x[1], reverse=True)
  score_50[key] = score[key][:50]
  score_1000[key] = score[key][:1000]
#printing top most 50 relevant documents for a query along with their scores:
for query in score_50.keys():
  print('Query number: {} score: {}'.format(query,score_50[query]))

with open('patterns.txt', 'r') as f:  
        file = f.read()
#file = file.lower()
file = file.split('\n')

query_answer = {}

for line in file:
    if query_answer.get(int(line.split()[0])) == None:
       query_answer[int(line.split()[0])] = [' '.join(line.split()[1:])]
    else:
       query_answer[int(line.split()[0])].append(' '.join(line.split()[1:]))

precision_50 = precision(score_50)
print(precision_50)
print('Mean Avg Precision is {}'.format(sum(list(precision_50.values()))/len(list(precision_50.values()))))

avg_dl =    len(corpus_combined)/Total_docs
score_bm25 = retrieval_BM25()
score_bm25_50 = {}
for key in score_bm25.keys():
    score_bm25_50[key]  =sorted(score_bm25[key].items(), key=lambda x: x[1], reverse=True)[:50]
#printing top most 50 relevant documents for a query along with their scores:
print('\n')
for query in score_bm25_50.keys():
  print('Query number: {} score: {}'.format(query,score_bm25_50[query]))

precision_50_bm25 = precision(score_bm25_50)

print(precision_50_bm25)
print('Mean Avg Precision is {}'.format(sum(list(precision_50_bm25.values()))/len(list(precision_50_bm25.values()))))

#Training the SVM. We have used fold 1 of MQ2008 dataset of LETOR 4.0 benchmark datasets. The data is trained on four features:
# Document Length, Term Frequency, TFIDF, BM25 
data = np.genfromtxt('dataset.csv',delimiter=',')
y = data[:,[-1,-2]]
y = y.astype('int32')
X= data[:,0:-2]
X_trans,y_trans = transform(X,y)
#print(index_comb[0])
#print(y.shape)
clf = LinearSVC()
clf.fit(X_trans,y_trans)
#print(X[0])
#print(y[0])

score_svm_50 = retrieval_SVM()
for query in score_svm_50.keys():
  print('Query number: {} score: {}'.format(query,score_svm_50[query]))

print('\n')
precision_50_svm = precision(score_svm_50)
print(precision_50_svm)
print('\nMean Avg Precision is {}'.format(sum(list(precision_50_svm.values()))/len(list(precision_50_svm.values()))))

sentence_raw,sentence_tokenized,avg_sl = get_sentence(score_bm25_50)
score_sentence = retrieval_BM25_sentence(sentence_raw,sentence_tokenized,avg_sl)
#print(score_sentence[1])
score_sentence_50 = {}
for key in score_sentence.keys():
  score_sentence_50[key]=sorted(score_sentence[key].items(), key=lambda x: x[1], reverse=True)[:50]

sentence_50  = {}
for query in score_sentence_50.keys():
  sentence_50[query] = [(sentence_raw[query][id],score) for id,score in score_sentence_50[query] ]
print(score_sentence_50[1])
print('\n******************************\n')
print(sentence_50[1])
print('\nMean Reciprocal Rank for bm25 is ',MRR(score_sentence_50))

sentence_raw,sentence_tokenized,avg_sl = get_sentence(score_svm_50)
score_sentence_BM25 = retrieval_BM25_sentence(sentence_raw,sentence_tokenized,avg_sl)
score_sentence_SVM_50 = retrieval_SVM_sentence(sentence_raw,sentence_tokenized,avg_sl,score_sentence_BM25)
sentence_SVM_50  = {}
for query in score_sentence_SVM_50.keys():
  sentence_SVM_50[query] = [(sentence_raw[query][id],score) for id,score in score_sentence_SVM_50[query] ]
print(score_sentence_SVM_50[3])
print('\n******************************\n')
print(sentence_SVM_50[3])
print('\nMean Reciprocal Rank for SVM is ',MRR(score_sentence_SVM_50))

# @Bhuvanesh:  IGNORE everything below this line. Also I am writing documentation for the code. 
# You just fill in the evaluation section where you mention results(MAP,MRR) for each model(Baseline,BM25,SVM)

print(len([x for x in precision_50.values() if x!=0]))

X = []
index_doc = {}
for doc_id,_ in score_1000[1]:
  feauture_tfidf = 0
  for term in queries[1].keys():
      feauture_tfidf+=  term_frequency.get(doc_id,{}).get(term,0)*idf.get(term,0)
  #feature_dl = len(corpus[doc_id])
  feature_BM25 =  score_bm25[1][doc_id]
  X.append([feauture_tfidf,feature_BM25])
  index_doc[len(X)-1] = doc_id
array = np.array(X)
mat = np.matrix(array)
with open('test.txt','wb') as f:
    for line in mat:
        np.savetxt(f, line,fmt='%.3f')

X  = np.random.randn(100,3)
y = np.random.randint(0,2,100)
clf.fit(X,y)

print(queries[100])